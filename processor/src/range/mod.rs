use super::{
    trace::{build_lookup_table_row_values, LookupTableRow, NUM_RAND_ROWS},
    utils::uninit_vector,
    BTreeMap, ColMatrix, Felt, FieldElement, RangeCheckTrace, Vec, ONE, ZERO,
};
use core::cmp::Ordering;

mod aux_trace;
pub use aux_trace::AuxTraceBuilder;

mod request;
use request::CycleRangeChecks;

#[cfg(test)]
mod tests;

// RANGE CHECKER
// ================================================================================================

/// Range checker for the VM.
///
/// This component is responsible for building an execution trace for all 16-bit range checks
/// performed by the VM. Thus, the [RangeChecker] doesn't actually check if a given value fits
/// into 16-bits, but rather keeps track of all 16-bit range checks performed by the VM.
///
/// ## Execution trace
/// Execution trace generated by the range checker consists of 3 columns. Conceptually, the table
/// starts with value 0 and end with value 65535.
///
/// The layout illustrated below.
///
///    s0     s1     v
/// ├─────┴──────┴─────┤
///
/// In the above, the meaning of the columns is as follows:
/// - Column `v` contains the value being range-checked where `v` must be a 16-bit value. The
///   values must be in increasing order and the jump allowed between two values should be a power
///   of 3 less than or equal to 3^7, and duplicates are allowed.
/// - Column `s0` and `s1` specify how many lookups are to be included for a given value.
///   Specifically: (0, 0) means no lookups, (1, 0) means one lookup, (0, 1), means two lookups,
///   and (1, 1) means four lookups.
///
/// Thus, for example, if a value was range-checked just once, we'll need to add a single row to
/// the table with (s0, s1, v) set to (1, 0, v), where v is the value.
///
/// If, on the other hand, the value was range-checked 5 times, we'll need two rows in the table:
/// (1, 1, v) and (1, 0, v). The first row specifies that there were 4 lookups and the second
/// row add the fifth lookup.
pub struct RangeChecker {
    /// Tracks lookup count for each checked value.
    lookups: BTreeMap<u16, usize>,
    // Range check lookups performed by all user operations, grouped and sorted by clock cycle. Each
    // cycle is mapped to a single CycleRangeChecks instance which includes lookups from the stack,
    // memory, or both.
    cycle_range_checks: BTreeMap<u32, CycleRangeChecks>,
}

impl RangeChecker {
    // CONSTRUCTOR
    // --------------------------------------------------------------------------------------------
    /// Returns a new [RangeChecker] instantiated with an empty lookup table.
    pub fn new() -> Self {
        let mut lookups = BTreeMap::new();
        // we need to make sure that the first row after the padded section and the last row of the
        // range checker table are initialized. this simplifies trace table building later on.
        lookups.insert(0, 0);
        lookups.insert(u16::MAX, 0);
        Self {
            lookups,
            cycle_range_checks: BTreeMap::new(),
        }
    }

    // TRACE MUTATORS
    // --------------------------------------------------------------------------------------------
    /// Adds the specified value to the trace of this range checker's lookups.
    pub fn add_value(&mut self, value: u16) {
        self.lookups.entry(value).and_modify(|v| *v += 1).or_insert(1);
    }

    /// Adds range check lookups from the [Stack] to this [RangeChecker] instance. Stack lookups are
    /// guaranteed to be added at unique clock cycles, since operations are sequential and no range
    /// check lookups are added before or during the stack operation processing.
    pub fn add_stack_checks(&mut self, clk: u32, values: &[u16; 4]) {
        self.add_value(values[0]);
        self.add_value(values[1]);
        self.add_value(values[2]);
        self.add_value(values[3]);

        // Stack operations are added before memory operations at unique clock cycles.
        self.cycle_range_checks.insert(clk, CycleRangeChecks::new_from_stack(values));
    }

    /// Adds range check lookups from [Memory] to this [RangeChecker] instance. Memory lookups are
    /// always added after all stack lookups have completed, since they are processed during trace
    /// finalization.
    pub fn add_mem_checks(&mut self, clk: u32, values: &[u16; 2]) {
        self.add_value(values[0]);
        self.add_value(values[1]);

        self.cycle_range_checks
            .entry(clk)
            .and_modify(|entry| entry.add_memory_checks(values))
            .or_insert_with(|| CycleRangeChecks::new_from_memory(values));
    }

    // EXECUTION TRACE GENERATION (INTERNAL)
    // --------------------------------------------------------------------------------------------

    /// Converts this [RangeChecker] into an execution trace with 3 columns and the number of rows
    /// specified by the `target_len` parameter.
    ///
    /// If the number of rows need to represent execution trace of this range checker is smaller
    /// than `target_len` parameter, the trace is padded with extra rows.
    ///
    /// `num_rand_rows` indicates the number of rows at the end of the trace which will be
    /// overwritten with random values. Values in these rows are not initialized.
    ///
    /// # Panics
    /// Panics if `target_len` is not a power of two or is smaller than the trace length needed
    /// to represent all lookups in this range checker.
    pub fn into_trace_with_table(
        self,
        trace_len: usize,
        target_len: usize,
        num_rand_rows: usize,
    ) -> RangeCheckTrace {
        assert!(target_len.is_power_of_two(), "target trace length is not a power of two");

        // determine the length of the trace required to support all the lookups in this range
        // checker, and make sure this length is smaller than or equal to the target trace length,
        // accounting for rows with random values.
        assert!(trace_len + num_rand_rows <= target_len, "target trace length too small");

        // allocated memory for the trace; this memory is un-initialized but this is not a problem
        // because we'll overwrite all values in it anyway.
        let mut trace = unsafe {
            [uninit_vector(target_len), uninit_vector(target_len), uninit_vector(target_len)]
        };
        // Allocate uninitialized memory for accumulating the precomputed auxiliary column hints.
        let mut row_flags = unsafe { uninit_vector(target_len) };

        // determine the number of padding rows needed to get to target trace length and pad the
        // table with the required number of rows.
        let num_padding_rows = target_len - trace_len - num_rand_rows;
        trace[0][..num_padding_rows].fill(ZERO);
        trace[1][..num_padding_rows].fill(ZERO);
        trace[2][..num_padding_rows].fill(ZERO);

        // Initialize the padded rows of the auxiliary column hints with the default flag, F0,
        // indicating s0 = s1 = ZERO.
        row_flags[..num_padding_rows].fill(RangeCheckFlag::F0);

        // build the trace table
        let mut i = num_padding_rows;
        let mut prev_value = 0u16;
        for (&value, &num_lookups) in self.lookups.iter() {
            write_rows(&mut trace, &mut i, num_lookups, value, prev_value, &mut row_flags);
            prev_value = value;
        }

        // pad the trace with an extra row of 0 lookups for u16::MAX so that when b_range is built
        // there is space for the inclusion of u16::MAX range check lookups before the trace ends.
        // (When there is data at the end of the main trace, auxiliary bus columns always need to be
        // one row longer than the main trace, since values in the bus column are based on data from
        // the "current" row of the main trace but placed into the "next" row of the bus column.)
        write_value(&mut trace, &mut i, 0, (u16::MAX).into(), &mut row_flags);

        RangeCheckTrace {
            trace,
            aux_builder: AuxTraceBuilder::new(self.cycle_range_checks, row_flags, num_padding_rows),
        }
    }

    // PUBLIC ACCESSORS
    // --------------------------------------------------------------------------------------------

    /// Returns the number of rows needed to support all 16-bit lookups requested by the VM.
    pub fn get_number_range_checker_rows(&self) -> usize {
        // pad the trace length by one, to account for an extra row of the u16::MAX value at the end
        // of the trace, required for building the `b_range` column.
        let mut num_rows = 1;

        let mut prev_value = 0u16;
        for (&value, &num_lookups) in self.lookups.iter() {
            // determine how many lookup rows we need for this value
            num_rows += lookups_to_rows(num_lookups);
            // determine the delta between this and the previous value. we need to know this delta
            // to determine if we need to insert any "bridge" rows to the  table, this is needed
            // since the gap between two values in the range checker can only be a power of 3 less
            // than or equal to 3^7.
            let delta = value - prev_value;
            num_rows += get_num_bridge_rows(delta);
            prev_value = value;
        }
        num_rows
    }

    // TEST HELPERS
    // --------------------------------------------------------------------------------------------

    /// Returns length of execution trace required to describe all 16-bit range checks performed
    /// by the VM.
    #[cfg(test)]
    pub fn trace_len(&self) -> usize {
        self.get_number_range_checker_rows()
    }

    /// Converts this [RangeChecker] into an execution trace with 3 columns and the number of rows
    /// specified by the `target_len` parameter.
    ///
    /// Wrapper for [`RangeChecker::into_trace_with_table`].
    #[cfg(test)]
    pub fn into_trace(self, target_len: usize, num_rand_rows: usize) -> RangeCheckTrace {
        let table_len = self.get_number_range_checker_rows();
        self.into_trace_with_table(table_len, target_len, num_rand_rows)
    }
}

impl Default for RangeChecker {
    fn default() -> Self {
        Self::new()
    }
}

// RANGE CHECKER ROWS
// ================================================================================================

/// A precomputed hint value that can be used to help construct the execution trace for the
/// auxiliary column b_range used for multiset checks. The hint is a precomputed flag value based
/// on the selectors s0 and s1 in the trace.
#[derive(Debug, PartialEq, Eq, Clone)]
pub enum RangeCheckFlag {
    F0,
    F1,
    F2,
    F3,
}

impl RangeCheckFlag {
    /// Reduces this row to a single field element in the field specified by E. This requires
    /// at least 1 alpha value.
    pub fn to_value<E: FieldElement<BaseField = Felt>>(&self, value: Felt, alphas: &[E]) -> E {
        let alpha: E = alphas[0];

        match self {
            RangeCheckFlag::F0 => E::ONE,
            RangeCheckFlag::F1 => alpha + value.into(),
            RangeCheckFlag::F2 => (alpha + value.into()).square(),
            RangeCheckFlag::F3 => ((alpha + value.into()).square()).square(),
        }
    }
}

// HELPER FUNCTIONS
// ================================================================================================

/// Returns the number of rows needed to perform the specified number of lookups for an 8-bit
/// value. Note that even if the number of lookups is 0, at least one row is required. This is
/// because for an 8-bit table, rows must contain contiguous values.
///
/// The number of rows is determined as follows:
/// - First we compute the number of rows for 4 lookups per row.
/// - Then we compute the number of rows for 2 lookups per row.
/// - Then, we compute the number of rows for a single lookup per row.
///
/// The return value is the sum of these three values.
fn lookups_to_rows(num_lookups: usize) -> usize {
    if num_lookups == 0 {
        1
    } else {
        let (num_rows4, num_lookups) = div_rem(num_lookups, 4);
        let (num_rows2, num_rows1) = div_rem(num_lookups, 2);
        num_rows4 + num_rows2 + num_rows1
    }
}

/// Calculates the number of bridge rows that are need to be added to the trace between two values
/// to be range checked.
pub fn get_num_bridge_rows(delta: u16) -> usize {
    let mut exp = 7;
    let mut gap = delta;
    let mut bridge_rows = 0_usize;
    let mut stride = 3_u16.pow(exp);
    while gap > 0 {
        if gap >= stride {
            bridge_rows += 1;
            gap -= stride;
        } else {
            exp = exp.saturating_sub(1);
            stride = 3_u16.pow(exp);
        }
    }
    bridge_rows.saturating_sub(1)
}

/// Adds a row for the values to be range checked. In case the difference between the current and
/// next value is not a power of 3, this function will add additional bridge rows to the trace.
fn write_rows(
    trace: &mut [Vec<Felt>],
    step: &mut usize,
    num_lookups: usize,
    value: u16,
    prev_value: u16,
    row_flags: &mut [RangeCheckFlag],
) {
    let mut exp = 7;
    let mut gap = value - prev_value;
    let mut prev_val = prev_value;
    let mut stride = 3_u16.pow(exp);
    if gap == 0 {
        write_value(trace, step, num_lookups, value as u64, row_flags);
    }
    while gap > 0 {
        match gap.cmp(&stride) {
            Ordering::Equal => {
                gap -= stride;
                write_value(trace, step, num_lookups, value as u64, row_flags);
            }
            Ordering::Greater => {
                gap -= stride;
                prev_val += stride;
                write_value(trace, step, 0, prev_val as u64, row_flags);
            }
            Ordering::Less => {
                exp = exp.saturating_sub(1);
                stride = 3_u16.pow(exp);
            }
        }
    }
}

/// Populates the trace with the rows needed to support the specified number of lookups against
/// the specified value.
fn write_value(
    trace: &mut [Vec<Felt>],
    step: &mut usize,
    num_lookups: usize,
    value: u64,
    row_flags: &mut [RangeCheckFlag],
) {
    // if the number of lookups is 0, only one trace row is required
    if num_lookups == 0 {
        row_flags[*step] = RangeCheckFlag::F0;
        write_trace_row(trace, step, ZERO, ZERO, value);
        return;
    }

    // write rows which can support 4 lookups per row
    let (num_rows, num_lookups) = div_rem(num_lookups, 4);
    for _ in 0..num_rows {
        row_flags[*step] = RangeCheckFlag::F3;
        write_trace_row(trace, step, ONE, ONE, value);
    }

    // write rows which can support 2 lookups per row
    let (num_rows, num_lookups) = div_rem(num_lookups, 2);
    for _ in 0..num_rows {
        row_flags[*step] = RangeCheckFlag::F2;
        write_trace_row(trace, step, ZERO, ONE, value);
    }

    // write rows which can support only one lookup per row
    for _ in 0..num_lookups {
        row_flags[*step] = RangeCheckFlag::F1;
        write_trace_row(trace, step, ONE, ZERO, value);
    }
}

/// Populates a single row at the specified step in the trace table.
fn write_trace_row(trace: &mut [Vec<Felt>], step: &mut usize, s0: Felt, s1: Felt, value: u64) {
    trace[0][*step] = s0;
    trace[1][*step] = s1;
    trace[2][*step] = Felt::new(value);
    *step += 1;
}

/// Returns quotient and remainder of dividing the provided value by the divisor.
fn div_rem(value: usize, divisor: usize) -> (usize, usize) {
    let q = value / divisor;
    let r = value % divisor;
    (q, r)
}
