# ===== ADDITION ==================================================================================

#! Performs addition of two unsigned 128 bit integers preserving the overflow.
#!
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [overflow, c0, c1, c2, c3, ...], where c = (a + b) % 2^128
pub proc overflowing_add(b: u128, a: u128) -> (i1, u128)
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    # Output: [overflow, c0, c1, c2, c3, ...]
    #
    # Following u64 pattern: keep sums at bottom, carries at top

    # Step 1: a0 + b0
    movup.4                     # [a0, b0, b1, b2, b3, a1, a2, a3]
    u32widening_add             # [c0, carry1, b1, b2, b3, a1, a2, a3]
    movdn.7                     # [carry1, b1, b2, b3, a1, a2, a3, c0]

    # Step 2: carry1 + a1 + b1
    movup.4                     # [a1, carry1, b1, b2, b3, a2, a3, c0]
    movup.2                     # [b1, a1, carry1, b2, b3, a2, a3, c0]
    u32widening_add3            # [c1, carry2, b2, b3, a2, a3, c0]
    movdn.6                     # [carry2, b2, b3, a2, a3, c0, c1]

    # Step 3: carry2 + a2 + b2
    movup.3                     # [a2, carry2, b2, b3, a3, c0, c1]
    movup.2                     # [b2, a2, carry2, b3, a3, c0, c1]
    u32widening_add3            # [c2, carry3, b3, a3, c0, c1]
    movdn.5                     # [carry3, b3, a3, c0, c1, c2]

    # Step 4: carry3 + a3 + b3
    movup.2                     # [a3, carry3, b3, c0, c1, c2]
    movup.2                     # [b3, a3, carry3, c0, c1, c2]
    u32widening_add3            # [c3, overflow, c0, c1, c2]

    # Current: [c3, overflow, c0, c1, c2]
    # Target:  [overflow, c0, c1, c2, c3]
    # Move c3 from position 0 to position 4
    movdn.4                     # [overflow, c0, c1, c2, c3]
end

#! Performs addition of two unsigned 128 bit integers preserving the overflow with sum on top.
#!
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, overflow, ...], where c = (a + b) % 2^128
pub proc widening_add(b: u128, a: u128) -> (u128, i1)
    exec.overflowing_add
    movdn.4
end

#! Performs addition of two unsigned 128 bit integers discarding the overflow.
#!
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = (a + b) % 2^128
pub proc wrapping_add(b: u128, a: u128) -> u128
    exec.overflowing_add
    drop
end

# ===== SUBTRACTION ===============================================================================

#! Performs subtraction of two unsigned 128 bit integers preserving the underflow.
#!
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [underflow, c0, c1, c2, c3, ...], where c = (a - b) % 2^128
pub proc overflowing_sub(b: u128, a: u128) -> (i1, u128)
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    # Following u64 pattern: move a limbs to front, then subtract limb by limb
    # u32overflowing_sub computes: second - top = [borrow, result]

    # Move a to front: [b...] [a...] -> [a...] [b...]
    movup.7 movup.7 movup.7 movup.7
                                # [a0, a1, a2, a3, b0, b1, b2, b3]

    # Step 1: a0 - b0
    movup.4                     # [b0, a0, a1, a2, a3, b1, b2, b3]
    u32overflowing_sub          # [borrow1, c0, a1, a2, a3, b1, b2, b3]
    movdn.7                     # [c0, a1, a2, a3, b1, b2, b3, borrow1]

    # Step 2: a1 - b1 - borrow1
    movup.4                     # [b1, c0, a1, a2, a3, b2, b3, borrow1]
    movup.2                     # [a1, b1, c0, a2, a3, b2, b3, borrow1]
    swap                        # [b1, a1, c0, a2, a3, b2, b3, borrow1]
    u32overflowing_sub          # [borrow2a, diff_ml, c0, a2, a3, b2, b3, borrow1]
    movup.7                     # [borrow1, borrow2a, diff_ml, c0, a2, a3, b2, b3]
    movup.2                     # [diff_ml, borrow1, borrow2a, c0, a2, a3, b2, b3]
    swap                        # [borrow1, diff_ml, borrow2a, c0, a2, a3, b2, b3]
    u32overflowing_sub          # [borrow2b, c1, borrow2a, c0, a2, a3, b2, b3]
    movup.2                     # [borrow2a, borrow2b, c1, c0, a2, a3, b2, b3]
    or                          # [borrow2, c1, c0, a2, a3, b2, b3]
    movdn.6                     # [c1, c0, a2, a3, b2, b3, borrow2]

    # Step 3: a2 - b2 - borrow2
    movup.4                     # [b2, c1, c0, a2, a3, b3, borrow2]
    movup.3                     # [a2, b2, c1, c0, a3, b3, borrow2]
    swap                        # [b2, a2, c1, c0, a3, b3, borrow2]
    u32overflowing_sub          # [borrow3a, diff_mh, c1, c0, a3, b3, borrow2]
    movup.6                     # [borrow2, borrow3a, diff_mh, c1, c0, a3, b3]
    movup.2                     # [diff_mh, borrow2, borrow3a, c1, c0, a3, b3]
    swap                        # [borrow2, diff_mh, borrow3a, c1, c0, a3, b3]
    u32overflowing_sub          # [borrow3b, c2, borrow3a, c1, c0, a3, b3]
    movup.2                     # [borrow3a, borrow3b, c2, c1, c0, a3, b3]
    or                          # [borrow3, c2, c1, c0, a3, b3]
    movdn.5                     # [c2, c1, c0, a3, b3, borrow3]

    # Step 4: a3 - b3 - borrow3
    movup.4                     # [b3, c2, c1, c0, a3, borrow3]
    movup.4                     # [a3, b3, c2, c1, c0, borrow3]
    swap                        # [b3, a3, c2, c1, c0, borrow3]
    u32overflowing_sub          # [borrow4a, diff_hh, c2, c1, c0, borrow3]
    movup.5                     # [borrow3, borrow4a, diff_hh, c2, c1, c0]
    movup.2                     # [diff_hh, borrow3, borrow4a, c2, c1, c0]
    swap                        # [borrow3, diff_hh, borrow4a, c2, c1, c0]
    u32overflowing_sub          # [borrow4b, c3, borrow4a, c2, c1, c0]
    movup.2                     # [borrow4a, borrow4b, c3, c2, c1, c0]
    or                          # [underflow, c3, c2, c1, c0]

    # Rearrange: [underflow, c3, c2, c1, c0] -> [underflow, c0, c1, c2, c3]
    movdn.4                     # [c3, c2, c1, c0, underflow]
    movdn.3                     # [c2, c1, c0, c3, underflow]
    movdn.2                     # [c1, c0, c2, c3, underflow]
    swap                        # [c0, c1, c2, c3, underflow]
    movup.4                     # [underflow, c0, c1, c2, c3]
end

#! Performs subtraction of two unsigned 128 bit integers discarding the underflow.
#!
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = (a - b) % 2^128
pub proc wrapping_sub(b: u128, a: u128) -> u128
    exec.overflowing_sub
    drop
end

# ===== MULTIPLICATION ============================================================================

#! Performs multiplication of two unsigned 128 bit integers preserving the overflow.
#!
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [overflow, c0, c1, c2, c3, ...], where c = (a * b) % 2^128
#!
#! Schoolbook multiplication (LE layout with low limbs on top):
#!
#!                 a0    a1    a2    a3
#!               x b0    b1    b2    b3
#! -------------------------------------------
#!  (position)     0       1       2       3       4       5       6
#!
#! Partial products contributing to each position:
#!   c0 (pos 0): a0*b0
#!   c1 (pos 1): a1*b0 + a0*b1 + carries from pos 0
#!   c2 (pos 2): a2*b0 + a1*b1 + a0*b2 + carries from pos 1
#!   c3 (pos 3): a3*b0 + a2*b1 + a1*b2 + a0*b3 + carries from pos 2
#!   overflow (pos 4+): a3*b1 + a2*b2 + a1*b3 + carries from pos 3
#!                    + a3*b2 + a2*b3 (pos 5)
#!                    + a3*b3 (pos 6)
#!
pub proc overflowing_mul(b: u128, a: u128) -> (i1, u128)
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    # Position:  0     1     2     3     4     5     6     7
    #
    # u32widening_mul: [b, a] -> [lo, hi] computes a*b
    # u32widening_madd: [c, b, a] -> [lo, hi] computes a*b+c

    # Move a limbs to front for easier access
    movup.7 movup.7 movup.7 movup.7
    # Now: [a0, a1, a2, a3, b0, b1, b2, b3, ...]
    # Position: 0     1     2     3     4     5     6     7

    # === Column 0: c0 ===
    # a0 * b0
    dup.4 dup.1                 # [a0, b0, a0, a1, a2, a3, b0, b1, b2, b3]
    u32widening_mul             # [c0, o0, a0, a1, a2, a3, b0, b1, b2, b3]
    movdn.9                     # [o0, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    # Stack: [o0, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    # Position: 0   1     2     3     4     5     6     7     8     9

    # === Column 1: c1 ===
    # a1 * b0 + o0: [c, b, a] for madd where c=o0, b=b0, a=a1
    dup.5 dup.3                 # [a1, b0, o0, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    u32widening_madd            # [p1, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0]

    # a0 * b1 + p1: [c, b, a] where c=p1, b=b1, a=a0
    dup.7 dup.3                 # [a0, b1, p1, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    u32widening_madd            # [c1, o1b, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    movdn.11                    # [o1b, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1]
    # Sum o1a + o1b, tracking carry for column 2
    u32widening_add             # [o1_sum, o1_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1]
    swap movdn.11               # [o1_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    # Stack: [o1_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    # Position:   0      1     2     3     4     5     6     7     8     9    10        11

    # === Column 2: c2 ===
    # a2 * b0 + o1_sum: [c, b, a] where c=o1_sum, b=b0, a=a2
    dup.5 dup.4                 # [a2, b0, o1_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    u32widening_madd            # [p2a, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]

    # a1 * b1 + p2a
    dup.7 dup.4                 # [a1, b1, p2a, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    u32widening_madd            # [p2b, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]

    # a0 * b2 + p2b
    dup.9 dup.4                 # [a0, b2, p2b, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    u32widening_madd            # [c2, o2c, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    movdn.13                    # [o2c, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2, o1_carry]
    # Stack has 15 elements: o1_carry at position 14, c2 at position 13
    # Sum o2a + o2b + o2c + o1_carry for carry into column 3
    u32widening_add3            # [o2_partial, o2_carry1, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2, o1_carry]
    # Stack has 14 elements: o1_carry at position 13, c2 at position 12
    movup.13                    # [o1_carry, o2_partial, o2_carry1, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2] (14 elements)
    u32widening_add             # [o2_sum, o2_carry2, o2_carry1, a0, ...] (13 elements)
    swap movup.2 add            # [o2_total_carry, o2_sum, a0, ...] (12 elements)
    swap                        # [o2_sum, o2_total_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Stack: [o2_sum, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:   0        1      2     3     4     5     6     7     8     9    10    11    12

    # === Column 3: c3 ===
    # Stack: [o2_sum, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:   0        1      2     3     4     5     6     7     8     9    10    11    12
    # a3 * b0 + o2_sum: [c, b, a] where c=o2_sum, b=b0, a=a3
    dup.6 dup.6                 # [a3, b0, o2_sum, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32widening_madd            # [p3a, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # a2 * b1 + p3a
    dup.8 dup.6                 # [a2, b1, p3a, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32widening_madd            # [p3b, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # a1 * b2 + p3b
    dup.10 dup.6                # [a1, b2, p3b, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32widening_madd            # [p3c, o3c, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # a0 * b3 + p3c
    dup.12 dup.6                # [a0, b3, p3c, o3c, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32widening_madd            # [c3, o3d, o3c, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # Stack: [c3, o3d, o3c, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position: 0     1    2    3    4      5       6     7     8     9    10    11    12    13    14    15    16
    # 17 elements

    # === Column 4+ (overflow): compute products that go directly to overflow ===
    # Products for position 4: a3*b1, a2*b2, a1*b3
    # Products for position 5: a3*b2, a2*b3
    # Products for position 6: a3*b3
    # We only need to detect if overflow is non-zero.
    # Note: the `or` instruction requires binary operands, so we convert each term to boolean.

    # Stack: [c3, o3d, o3c, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position: 0     1    2    3    4      5       6     7     8     9    10    11    12    13    14    15    16

    # Start with carries from column 3: (o3a + o3b + o3c + o3d + o2_carry) != 0
    swap                        # [o3d, c3, o3c, o3b, o3a, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    movup.2 add                 # [o3d+o3c, c3, o3b, o3a, o2_carry, ...] 16 elements
    movup.2 add                 # [sum1, c3, o3a, o2_carry, ...] 15 elements
    movup.2 add                 # [sum2, c3, o2_carry, a0, ...] 14 elements
    movup.2 add                 # [carry_sum, c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    neq.0                       # [overflow_acc, c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Stack: [overflow_acc (bool), c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:        0          1     2     3     4     5     6     7     8     9    10    11    12
    # 13 elements

    # Now compute the direct overflow products, converting each to boolean before OR
    # Stack: [overflow_acc, c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:    0          1     2     3     4     5     6     7     8     9    10    11    12

    # a3 * b1 (position 4): a3 at pos 5, b1 at pos 7
    dup.7 dup.6                 # [a3, b1, overflow_acc, c3, ...]
    u32widening_mul             # [lo, hi, overflow_acc, c3, ...]
    add neq.0                   # [p_nonzero, overflow_acc, c3, ...] (lo+hi != 0 iff either is nonzero)
    or                          # [overflow_acc, c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # a2 * b2 (position 4): a2 at pos 4, b2 at pos 8
    dup.8 dup.5                 # [a2, b2, overflow_acc, c3, ...]
    u32widening_mul             # [lo, hi, overflow_acc, c3, ...]
    add neq.0 or                # [overflow_acc, c3, ...]

    # a1 * b3 (position 4): a1 at pos 3, b3 at pos 9
    dup.9 dup.4                 # [a1, b3, overflow_acc, c3, ...]
    u32widening_mul             # [lo, hi, overflow_acc, c3, ...]
    add neq.0 or                # [overflow_acc, c3, ...]

    # a3 * b2 (position 5): a3 at pos 5, b2 at pos 8
    dup.8 dup.6                 # [a3, b2, overflow_acc, c3, ...]
    u32widening_mul             # [lo, hi, overflow_acc, c3, ...]
    add neq.0 or                # [overflow_acc, c3, ...]

    # a2 * b3 (position 5): a2 at pos 4, b3 at pos 9
    dup.9 dup.5                 # [a2, b3, overflow_acc, c3, ...]
    u32widening_mul             # [lo, hi, overflow_acc, c3, ...]
    add neq.0 or                # [overflow_acc, c3, ...]

    # a3 * b3 (position 6): a3 at pos 5, b3 at pos 9
    dup.9 dup.6                 # [a3, b3, overflow_acc, c3, ...]
    u32widening_mul             # [lo, hi, overflow_acc, c3, ...]
    add neq.0 or                # [overflow, c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # Clean up: remove a and b limbs, keep only overflow, c3, c0, c1, c2
    movup.2 drop                # [overflow, c3, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, a3, b0, b1, b2, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, b0, b1, b2, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, b1, b2, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, b2, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, b3, c0, c1, c2]
    movup.2 drop                # [overflow, c3, c0, c1, c2]

    # Rearrange: [overflow, c3, c0, c1, c2] -> [overflow, c0, c1, c2, c3]
    swap                        # [c3, overflow, c0, c1, c2]
    movdn.4                     # [overflow, c0, c1, c2, c3]
end

#! Performs multiplication of two unsigned 128 bit integers preserving the overflow with sum on top.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, overflow, ...], where c = (a * b) % 2^128
pub proc widening_mul(b: u128, a: u128) -> (u128, i1)
    exec.overflowing_mul
    movdn.4
end

#! Performs multiplication of two unsigned 128 bit integers discarding the overflow.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition is as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = (a * b) % 2^128
#!
#! Uses schoolbook multiplication with u32wrapping_madd for products contributing to c3
#! since overflow there doesn't affect the result.
pub proc wrapping_mul(b: u128, a: u128) -> u128
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    # Position:  0     1     2     3     4     5     6     7
    #
    # u32widening_mul: [b, a] -> [lo, hi] computes a*b
    # u32widening_madd: [c, b, a] -> [lo, hi] computes a*b+c

    # Move a limbs to front for easier access
    movup.7 movup.7 movup.7 movup.7
    # Now: [a0, a1, a2, a3, b0, b1, b2, b3, ...]
    # Position: 0     1     2     3     4     5     6     7

    # === Column 0: c0 ===
    # a0 * b0
    dup.4 dup.1                 # [a0, b0, a0, a1, a2, a3, b0, b1, b2, b3]
    u32widening_mul             # [c0, o0, a0, a1, a2, a3, b0, b1, b2, b3]
    movdn.9                     # [o0, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    # Stack: [o0, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    # Position: 0   1     2     3     4     5     6     7     8     9

    # === Column 1: c1 ===
    # a1 * b0 + o0: [c, b, a] for madd where c=o0, b=b0, a=a1
    dup.5 dup.3                 # [a1, b0, o0, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    u32widening_madd            # [p1, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0]

    # a0 * b1 + p1: [c, b, a] where c=p1, b=b1, a=a0
    dup.7 dup.3                 # [a0, b1, p1, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    u32widening_madd            # [c1, o1b, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0]
    movdn.11                    # [o1b, o1a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1]
    # Sum o1a + o1b, tracking carry for column 2
    u32widening_add             # [o1_sum, o1_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1]
    swap movdn.11               # [o1_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    # Stack: [o1_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    # Position:   0      1     2     3     4     5     6     7     8     9    10        11

    # === Column 2: c2 ===
    # a2 * b0 + o1_sum: [c, b, a] where c=o1_sum, b=b0, a=a2
    dup.5 dup.4                 # [a2, b0, o1_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    u32widening_madd            # [p2a, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]

    # a1 * b1 + p2a
    dup.7 dup.4                 # [a1, b1, p2a, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    u32widening_madd            # [p2b, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]

    # a0 * b2 + p2b
    dup.9 dup.4                 # [a0, b2, p2b, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    u32widening_madd            # [c2, o2c, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, o1_carry]
    movdn.13                    # [o2c, o2b, o2a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2, o1_carry]
    # Stack has 15 elements: o1_carry at position 14, c2 at position 13
    # Sum o2a + o2b + o2c + o1_carry for carry into column 3
    u32widening_add3            # [o2_partial, o2_carry1, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2, o1_carry]
    # Stack has 14 elements: o1_carry at position 13, c2 at position 12
    movup.13                    # [o1_carry, o2_partial, o2_carry1, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32widening_add             # [o2_sum, o2_carry2, o2_carry1, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Combine carries: o2_carry1 + o2_carry2 (will be at most 2, fits in u32)
    swap movup.2 add            # [o2_total_carry, o2_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    swap                        # [o2_sum, o2_total_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Stack: [o2_sum, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:   0        1      2     3     4     5     6     7     8     9    10    11    12

    # === Column 3: c3 (overflow discarded) ===
    # Stack: [o2_sum, o2_carry, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:   0        1      2     3     4     5     6     7     8     9    10    11    12
    # 13 elements
    # For wrapping_mul, o2_carry represents overflow beyond 128 bits - discard it now
    swap drop                   # [o2_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Stack: [o2_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:   0      1     2     3     4     5     6     7     8     9    10    11
    # 12 elements

    # a3 * b0 + o2_sum
    dup.5 dup.5                 # dup.5 gets b0, then dup.5 gets a3
                                # [a3, b0, o2_sum, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32wrapping_madd            # [p3a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Stack: [p3a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position:  0    1     2     3     4     5     6     7     8     9    10    11
    # 12 elements

    # a2 * b1 + p3a
    dup.6 dup.4                 # dup.6 gets b1, then dup.4 gets a2
                                # [a2, b1, p3a, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32wrapping_madd            # [p3b, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # a1 * b2 + p3b
    dup.7 dup.3                 # dup.7 gets b2, then dup.3 gets a1
                                # [a1, b2, p3b, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32wrapping_madd            # [p3c, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]

    # a0 * b3 + p3c
    dup.8 dup.2                 # dup.8 gets b3, then dup.2 gets a0
                                # [a0, b3, p3c, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    u32wrapping_madd            # [c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Stack: [c3, a0, a1, a2, a3, b0, b1, b2, b3, c0, c1, c2]
    # Position: 0     1     2     3     4     5     6     7     8     9    10    11
    # 12 elements; extract c0, c1, c2 from positions 9, 10, 11
    movup.9                     # [c0, c3, a0, a1, a2, a3, b0, b1, b2, b3, c1, c2]
    movup.10                    # [c1, c0, c3, a0, a1, a2, a3, b0, b1, b2, b3, c2]
    movup.11                    # [c2, c1, c0, c3, a0, a1, a2, a3, b0, b1, b2, b3]
    movup.3                     # [c3, c2, c1, c0, a0, a1, a2, a3, b0, b1, b2, b3]
    # Stack: [c3, c2, c1, c0, a0, a1, a2, a3, b0, b1, b2, b3]
    # Position: 0     1     2     3     4     5     6     7     8     9    10    11
    # 12 elements; need to drop 8 elements (a and b limbs)
    swapw                       # [a0, a1, a2, a3, c3, c2, c1, c0, b0, b1, b2, b3]
    dropw                       # [c3, c2, c1, c0, b0, b1, b2, b3]
    swapw                       # [b0, b1, b2, b3, c3, c2, c1, c0]
    dropw                       # [c3, c2, c1, c0]

    # Rearrange from [c3, c2, c1, c0] to [c0, c1, c2, c3]
    movdn.3                     # [c2, c1, c0, c3]
    movdn.2                     # [c1, c0, c2, c3]
    swap                        # [c0, c1, c2, c3]
end

# ===== COMPARISONS ===============================================================================

#! Performs equality comparison of two unsigned 128 bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a == b, and 0 otherwise.
pub proc eq(b: u128, a: u128) -> i1
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    movup.4         # [a0, b0, b1, b2, b3, a1, a2, a3]
    eq              # [a0==b0, b1, b2, b3, a1, a2, a3]
    movup.4         # [a1, a0==b0, b1, b2, b3, a2, a3]
    movup.2         # [b1, a1, a0==b0, b2, b3, a2, a3]
    eq              # [a1==b1, a0==b0, b2, b3, a2, a3]
    and             # [eq01, b2, b3, a2, a3]
    movup.3         # [a2, eq01, b2, b3, a3]
    movup.2         # [b2, a2, eq01, b3, a3]
    eq              # [a2==b2, eq01, b3, a3]
    and             # [eq012, b3, a3]
    movup.2         # [a3, eq012, b3]
    movup.2         # [b3, a3, eq012]
    eq              # [a3==b3, eq012]
    and             # [result]
end

#! Performs inequality comparison of two unsigned 128 bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a != b, and 0 otherwise.
pub proc neq(b: u128, a: u128) -> i1
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    movup.4         # [a0, b0, b1, b2, b3, a1, a2, a3]
    neq             # [a0!=b0, b1, b2, b3, a1, a2, a3]
    movup.4         # [a1, a0!=b0, b1, b2, b3, a2, a3]
    movup.2         # [b1, a1, a0!=b0, b2, b3, a2, a3]
    neq             # [a1!=b1, a0!=b0, b2, b3, a2, a3]
    or              # [neq01, b2, b3, a2, a3]
    movup.3         # [a2, neq01, b2, b3, a3]
    movup.2         # [b2, a2, neq01, b3, a3]
    neq             # [a2!=b2, neq01, b3, a3]
    or              # [neq012, b3, a3]
    movup.2         # [a3, neq012, b3]
    movup.2         # [b3, a3, neq012]
    neq             # [a3!=b3, neq012]
    or              # [result]
end

#! Performs comparison to zero of an unsigned 128 bit integer.
#! The input value is assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a == 0, and 0 otherwise.
pub proc eqz(a: u128) -> i1
    # Input: [a0, a1, a2, a3, ...]
    eq.0            # [a0==0, a1, a2, a3]
    swap            # [a1, a0==0, a2, a3]
    eq.0            # [a1==0, a0==0, a2, a3]
    and             # [eq01, a2, a3]
    swap            # [a2, eq01, a3]
    eq.0            # [a2==0, eq01, a3]
    and             # [eq012, a3]
    swap            # [a3, eq012]
    eq.0            # [a3==0, eq012]
    and             # [result]
end

#! Performs less-than comparison of two unsigned 128 bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a < b, and 0 otherwise.
pub proc lt(b: u128, a: u128) -> i1
    # Use subtraction: a < b iff (a - b) underflows
    exec.overflowing_sub
    # Stack: [underflow, c0, c1, c2, c3, ...]
    # Drop the result, keep only underflow flag
    movdn.4                     # [c0, c1, c2, c3, underflow]
    drop drop drop drop         # [underflow]
end

#! Performs greater-than comparison of two unsigned 128 bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a > b, and 0 otherwise.
pub proc gt(b: u128, a: u128) -> i1
    # a > b iff b < a
    # Swap a and b, then check if b < a (underflow)
    movup.7 movup.7 movup.7 movup.7
    # Stack: [a0, a1, a2, a3, b0, b1, b2, b3, ...] - now b is "a" and a is "b" for sub
    exec.overflowing_sub
    # This computes b - a, underflow means b < a, i.e., a > b
    movdn.4
    drop drop drop drop
end

#! Performs less-than-or-equal comparison of two unsigned 128 bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a <= b, and 0 otherwise.
pub proc lte(b: u128, a: u128) -> i1
    exec.gt
    not
end

#! Performs greater-than-or-equal comparison of two unsigned 128 bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c, ...], where c = 1 when a >= b, and 0 otherwise.
pub proc gte(b: u128, a: u128) -> i1
    exec.lt
    not
end

#! Compares two unsigned 128 bit integers and drops the larger one from the stack.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = min(a, b).
pub proc min(b: u128, a: u128) -> u128
    # Duplicate both values for comparison
    dupw.1                      # [a0, a1, a2, a3, b0, b1, b2, b3, a0, a1, a2, a3, ...]
    dupw.1                      # [b0, b1, b2, b3, a0, a1, a2, a3, b0, b1, b2, b3, a0, a1, a2, a3, ...]
    exec.gt                     # [a>b, b0, b1, b2, b3, a0, a1, a2, a3, ...]
    # If a > b, keep b (first), else keep a (second)
    # cdrop: [cond, a, b] -> [a if cond else b]
    # We need to select between two u128 values
    dup                         # [a>b, a>b, b0, b1, b2, b3, a0, a1, a2, a3]
    movdn.5                     # [a>b, b0, b1, b2, b3, a>b, a0, a1, a2, a3]
    movup.6                     # [a0, a>b, b0, b1, b2, b3, a>b, a1, a2, a3]
    movup.2                     # [b0, a0, a>b, b1, b2, b3, a>b, a1, a2, a3]
    dup.2 cdrop                 # [c0, a>b, b1, b2, b3, a>b, a1, a2, a3]
    movdn.8                     # [a>b, b1, b2, b3, a>b, a1, a2, a3, c0]
    movup.5                     # [a1, a>b, b1, b2, b3, a>b, a2, a3, c0]
    movup.2                     # [b1, a1, a>b, b2, b3, a>b, a2, a3, c0]
    dup.2 cdrop                 # [c1, a>b, b2, b3, a>b, a2, a3, c0]
    movdn.7                     # [a>b, b2, b3, a>b, a2, a3, c0, c1]
    movup.4                     # [a2, a>b, b2, b3, a>b, a3, c0, c1]
    movup.2                     # [b2, a2, a>b, b3, a>b, a3, c0, c1]
    dup.2 cdrop                 # [c2, a>b, b3, a>b, a3, c0, c1]
    movdn.6                     # [a>b, b3, a>b, a3, c0, c1, c2]
    drop                        # [b3, a>b, a3, c0, c1, c2]
    movup.2                     # [a3, b3, a>b, c0, c1, c2]
    swap                        # [b3, a3, a>b, c0, c1, c2]
    movup.2 cdrop               # [c3, c0, c1, c2]
    movdn.3                     # [c0, c1, c2, c3]
end

#! Compares two unsigned 128 bit integers and drops the smaller one from the stack.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = max(a, b).
pub proc max(b: u128, a: u128) -> u128
    # Duplicate both values for comparison
    dupw.1                      # [a0, a1, a2, a3, b0, b1, b2, b3, a0, a1, a2, a3, ...]
    dupw.1                      # [b0, b1, b2, b3, a0, a1, a2, a3, b0, b1, b2, b3, a0, a1, a2, a3, ...]
    exec.lt                     # [a<b, b0, b1, b2, b3, a0, a1, a2, a3, ...]
    # If a < b, keep b (first), else keep a (second)
    dup                         # [a<b, a<b, b0, b1, b2, b3, a0, a1, a2, a3]
    movdn.5                     # [a<b, b0, b1, b2, b3, a<b, a0, a1, a2, a3]
    movup.6                     # [a0, a<b, b0, b1, b2, b3, a<b, a1, a2, a3]
    movup.2                     # [b0, a0, a<b, b1, b2, b3, a<b, a1, a2, a3]
    dup.2 cdrop                 # [c0, a<b, b1, b2, b3, a<b, a1, a2, a3]
    movdn.8                     # [a<b, b1, b2, b3, a<b, a1, a2, a3, c0]
    movup.5                     # [a1, a<b, b1, b2, b3, a<b, a2, a3, c0]
    movup.2                     # [b1, a1, a<b, b2, b3, a<b, a2, a3, c0]
    dup.2 cdrop                 # [c1, a<b, b2, b3, a<b, a2, a3, c0]
    movdn.7                     # [a<b, b2, b3, a<b, a2, a3, c0, c1]
    movup.4                     # [a2, a<b, b2, b3, a<b, a3, c0, c1]
    movup.2                     # [b2, a2, a<b, b3, a<b, a3, c0, c1]
    dup.2 cdrop                 # [c2, a<b, b3, a<b, a3, c0, c1]
    movdn.6                     # [a<b, b3, a<b, a3, c0, c1, c2]
    drop                        # [b3, a<b, a3, c0, c1, c2]
    movup.2                     # [a3, b3, a<b, c0, c1, c2]
    swap                        # [b3, a3, a<b, c0, c1, c2]
    movup.2 cdrop               # [c3, c0, c1, c2]
    movdn.3                     # [c0, c1, c2, c3]
end

# ===== BITWISE OPERATIONS ========================================================================

#! Performs bitwise AND of two unsigned 128-bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = a AND b.
pub proc and(b: u128, a: u128) -> u128
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    movup.4         # [a0, b0, b1, b2, b3, a1, a2, a3]
    u32and          # [c0, b1, b2, b3, a1, a2, a3]
    movup.4         # [a1, c0, b1, b2, b3, a2, a3]
    movup.2         # [b1, a1, c0, b2, b3, a2, a3]
    u32and          # [c1, c0, b2, b3, a2, a3]
    movup.4         # [a2, c1, c0, b2, b3, a3]
    movup.3         # [b2, a2, c1, c0, b3, a3]
    u32and          # [c2, c1, c0, b3, a3]
    movup.4         # [a3, c2, c1, c0, b3]
    movup.4         # [b3, a3, c2, c1, c0]
    u32and          # [c3, c2, c1, c0]
    movdn.3         # [c2, c1, c0, c3]
    movdn.2         # [c1, c0, c2, c3]
    swap            # [c0, c1, c2, c3]
end

#! Performs bitwise OR of two unsigned 128-bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = a OR b.
pub proc or(b: u128, a: u128) -> u128
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    movup.4         # [a0, b0, b1, b2, b3, a1, a2, a3]
    u32or           # [c0, b1, b2, b3, a1, a2, a3]
    movup.4         # [a1, c0, b1, b2, b3, a2, a3]
    movup.2         # [b1, a1, c0, b2, b3, a2, a3]
    u32or           # [c1, c0, b2, b3, a2, a3]
    movup.4         # [a2, c1, c0, b2, b3, a3]
    movup.3         # [b2, a2, c1, c0, b3, a3]
    u32or           # [c2, c1, c0, b3, a3]
    movup.4         # [a3, c2, c1, c0, b3]
    movup.4         # [b3, a3, c2, c1, c0]
    u32or           # [c3, c2, c1, c0]
    movdn.3         # [c2, c1, c0, c3]
    movdn.2         # [c1, c0, c2, c3]
    swap            # [c0, c1, c2, c3]
end

#! Performs bitwise XOR of two unsigned 128-bit integers.
#! The input values are assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [b0, b1, b2, b3, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = a XOR b.
pub proc xor(b: u128, a: u128) -> u128
    # Input: [b0, b1, b2, b3, a0, a1, a2, a3, ...]
    movup.4         # [a0, b0, b1, b2, b3, a1, a2, a3]
    u32xor          # [c0, b1, b2, b3, a1, a2, a3]
    movup.4         # [a1, c0, b1, b2, b3, a2, a3]
    movup.2         # [b1, a1, c0, b2, b3, a2, a3]
    u32xor          # [c1, c0, b2, b3, a2, a3]
    movup.4         # [a2, c1, c0, b2, b3, a3]
    movup.3         # [b2, a2, c1, c0, b3, a3]
    u32xor          # [c2, c1, c0, b3, a3]
    movup.4         # [a3, c2, c1, c0, b3]
    movup.4         # [b3, a3, c2, c1, c0]
    u32xor          # [c3, c2, c1, c0]
    movdn.3         # [c2, c1, c0, c3]
    movdn.2         # [c1, c0, c2, c3]
    swap            # [c0, c1, c2, c3]
end

#! Performs bitwise NOT of one unsigned 128-bit integer.
#! The input value is assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = NOT a.
pub proc not(a: u128) -> u128
    # Input: [a0, a1, a2, a3, ...]
    u32not          # [c0, a1, a2, a3]
    swap            # [a1, c0, a2, a3]
    u32not          # [c1, c0, a2, a3]
    swap            # [c0, c1, a2, a3]
    movup.2         # [a2, c0, c1, a3]
    u32not          # [c2, c0, c1, a3]
    movdn.2         # [c0, c1, c2, a3]
    movup.3         # [a3, c0, c1, c2]
    u32not          # [c3, c0, c1, c2]
    movdn.3         # [c0, c1, c2, c3]
end

#! Counts the number of leading zeros of one unsigned 128-bit integer.
#! The input value is assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [a0, a1, a2, a3, ...] -> [clz, ...], where clz is the number of leading zeros of value a.
pub proc clz(a: u128) -> u8
    # Input: [a0, a1, a2, a3, ...]
    # Leading zeros start from the most significant limb (a3)
    movup.3         # [a3, a0, a1, a2, ...]
    dup.0
    eq.0

    if.true         # a3 == 0
        drop        # [a0, a1, a2, ...]
        movup.2     # [a2, a0, a1, ...]
        dup.0
        eq.0

        if.true     # a2 == 0
            drop    # [a0, a1, ...]
            swap    # [a1, a0, ...]
            dup.0
            eq.0

            if.true # a1 == 0
                drop        # [a0, ...]
                u32clz
                add.96      # clz(a0) + 96
            else
                swap drop   # [a1, ...]
                u32clz
                add.64      # clz(a1) + 64
            end
        else
            movdn.2 drop drop   # [a2, ...]
            u32clz
            add.32              # clz(a2) + 32
        end
    else
        movdn.3 drop drop drop  # [a3, ...]
        u32clz                  # clz(a3)
    end
end

#! Counts the number of trailing zeros of one unsigned 128-bit integer.
#! The input value is assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [a0, a1, a2, a3, ...] -> [ctz, ...], where ctz is the number of trailing zeros of value a.
pub proc ctz(a: u128) -> u8
    # Input: [a0, a1, a2, a3, ...]
    # Trailing zeros start from the least significant limb (a0)
    dup.0
    eq.0

    if.true         # a0 == 0
        drop        # [a1, a2, a3, ...]
        dup.0
        eq.0

        if.true     # a1 == 0
            drop    # [a2, a3, ...]
            dup.0
            eq.0

            if.true # a2 == 0
                drop        # [a3, ...]
                u32ctz
                add.96      # ctz(a3) + 96
            else
                swap drop   # [a2, ...]
                u32ctz
                add.64      # ctz(a2) + 64
            end
        else
            movdn.2 drop drop   # [a1, ...]
            u32ctz
            add.32              # ctz(a1) + 32
        end
    else
        movdn.3 drop drop drop  # [a0, ...]
        u32ctz                  # ctz(a0)
    end
end

#! Counts the number of leading ones of one unsigned 128-bit integer.
#! The input value is assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [a0, a1, a2, a3, ...] -> [clo, ...], where clo is the number of leading ones of value a.
pub proc clo(a: u128) -> u8
    # Input: [a0, a1, a2, a3, ...]
    # Leading ones start from the most significant limb (a3)
    # 0xFFFFFFFF = 4294967295
    movup.3         # [a3, a0, a1, a2, ...]
    dup.0
    eq.4294967295

    if.true         # a3 == 0xFFFFFFFF
        drop        # [a0, a1, a2, ...]
        movup.2     # [a2, a0, a1, ...]
        dup.0
        eq.4294967295

        if.true     # a2 == 0xFFFFFFFF
            drop    # [a0, a1, ...]
            swap    # [a1, a0, ...]
            dup.0
            eq.4294967295

            if.true # a1 == 0xFFFFFFFF
                drop        # [a0, ...]
                u32clo
                add.96      # clo(a0) + 96
            else
                swap drop   # [a1, ...]
                u32clo
                add.64      # clo(a1) + 64
            end
        else
            movdn.2 drop drop   # [a2, ...]
            u32clo
            add.32              # clo(a2) + 32
        end
    else
        movdn.3 drop drop drop  # [a3, ...]
        u32clo                  # clo(a3)
    end
end

#! Counts the number of trailing ones of one unsigned 128-bit integer.
#! The input value is assumed to be represented using 32 bit limbs, but this is not checked.
#! Stack transition looks as follows:
#! [a0, a1, a2, a3, ...] -> [cto, ...], where cto is the number of trailing ones of value a.
pub proc cto(a: u128) -> u8
    # Input: [a0, a1, a2, a3, ...]
    # Trailing ones start from the least significant limb (a0)
    # 0xFFFFFFFF = 4294967295
    dup.0
    eq.4294967295

    if.true         # a0 == 0xFFFFFFFF
        drop        # [a1, a2, a3, ...]
        dup.0
        eq.4294967295

        if.true     # a1 == 0xFFFFFFFF
            drop    # [a2, a3, ...]
            dup.0
            eq.4294967295

            if.true # a2 == 0xFFFFFFFF
                drop        # [a3, ...]
                u32cto
                add.96      # cto(a3) + 96
            else
                swap drop   # [a2, ...]
                u32cto
                add.64      # cto(a2) + 64
            end
        else
            movdn.2 drop drop   # [a1, ...]
            u32cto
            add.32              # cto(a1) + 32
        end
    else
        movdn.3 drop drop drop  # [a0, ...]
        u32cto                  # cto(a0)
    end
end

# ===== SHIFT OPERATIONS ==========================================================================

#! Performs left shift of one unsigned 128-bit integer.
#! The input value to be shifted is assumed to be represented using 32 bit limbs, but this is not checked.
#! The shift value n should be in the range [0, 128), otherwise it will result in an error.
#! Stack transition looks as follows:
#! [n, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = (a << n) mod 2^128.
pub proc shl(n: u32, a: u128) -> u128
    # Strategy: compute 2^n as u128, then use wrapping_mul
    # For n < 64: 2^n fits in lower 64 bits
    # For n >= 64: 2^n is in upper 64 bits

    dup             # [n, n, a0, a1, a2, a3, ...]
    push.64
    u32lt           # [n<64, n, a0, a1, a2, a3, ...]

    if.true
        # n < 64: 2^n = [pow_lo, pow_hi, 0, 0]
        pow2            # [2^n, a0, a1, a2, a3, ...]
        u32split        # [pow_lo, pow_hi, a0, a1, a2, a3, ...]
        push.0 push.0   # [0, 0, pow_lo, pow_hi, a0, a1, a2, a3, ...]
        movup.3         # [pow_hi, 0, 0, pow_lo, a0, a1, a2, a3, ...]
        movup.3         # [pow_lo, pow_hi, 0, 0, a0, a1, a2, a3, ...]
        # Stack: [pow_lo, pow_hi, 0, 0, a0, a1, a2, a3, ...] - this is 2^n in u128 format
        # But wait, we need [b0, b1, b2, b3, a0, a1, a2, a3] for wrapping_mul
        # pow_lo=b0, pow_hi=b1, 0=b2, 0=b3
        # Current: [pow_lo, pow_hi, 0, 0, a0, a1, a2, a3] = [b0, b1, b2, b3, a0, a1, a2, a3]
        exec.wrapping_mul
    else
        # n >= 64: 2^n = [0, 0, pow_lo, pow_hi] where pow = 2^(n-64)
        push.64
        u32wrapping_sub # [n-64, a0, a1, a2, a3, ...]
        pow2            # [2^(n-64), a0, a1, a2, a3, ...]
        u32split        # [pow_lo, pow_hi, a0, a1, a2, a3, ...]
        # Need [0, 0, pow_lo, pow_hi, a0, a1, a2, a3]
        push.0 push.0   # [0, 0, pow_lo, pow_hi, a0, a1, a2, a3, ...]
        # Stack: [0, 0, pow_lo, pow_hi, a0, a1, a2, a3, ...] = [b0, b1, b2, b3, a0, a1, a2, a3]
        exec.wrapping_mul
    end
end

#! Performs right shift of one unsigned 128-bit integer.
#! The input value to be shifted is assumed to be represented using 32 bit limbs, but this is not checked.
#! The shift value n should be in the range [0, 128), otherwise it will result in an error.
#! Stack transition looks as follows:
#! [n, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = a >> n.
#!
#! This implementation correctly handles cross-limb bit transfers for arbitrary shift amounts.
pub proc shr(n: u32, a: u128) -> u128
    # Stack: [n, a0, a1, a2, a3, ...] (5 elements + padding)
    # Output: [c0, c1, c2, c3, ...] (4 elements + padding)
    #
    # Algorithm: k = n / 32 (limb shift), m = n % 32 (bit shift)
    # For each c_i = (a_{i+k} >> m) | (a_{i+k+1} << (32-m))

    # Handle n == 0: no shift needed
    dup eq.0
    if.true
        drop            # [a0, a1, a2, a3, ...]
    else
        # Compute k = n / 32 and m = n % 32
        # Stack: [n, a0, a1, a2, a3, ...]
        dup push.31 u32and      # [m, n, a0, a1, a2, a3, ...]
        swap push.5 u32shr      # [k, m, a0, a1, a2, a3, ...]

        # Branch based on k (0, 1, 2, 3, or >=4)
        dup push.4 u32lt
        if.true
            # k < 4: compute shifted result
            dup eq.0
            if.true
                # k=0: n in [1, 31], shift by m bits within limbs
                drop        # [m, a0, a1, a2, a3, ...]
                exec.shr_k0
            else
                dup eq.1
                if.true
                    # k=1: n in [32, 63]
                    drop    # [m, a0, a1, a2, a3, ...]
                    exec.shr_k1
                else
                    dup eq.2
                    if.true
                        # k=2: n in [64, 95]
                        drop    # [m, a0, a1, a2, a3, ...]
                        exec.shr_k2
                    else
                        # k=3: n in [96, 127]
                        drop    # [m, a0, a1, a2, a3, ...]
                        exec.shr_k3
                    end
                end
            end
        else
            # k >= 4: result is all zeros (n >= 128)
            drop drop       # [a0, a1, a2, a3, ...]
            drop drop drop drop
            push.0 push.0 push.0 push.0
        end
    end
end

#! Helper for shr with k=0 (shift by m bits, 0 < m < 32)
#! Input: [m, a0, a1, a2, a3, ...]
#! Output: [c0, c1, c2, c3, ...]
#! c_i = (a_i >> m) | (a_{i+1} << (32-m)), c3 = a3 >> m
proc shr_k0
    # Compute 2^(32-m) for left shift operations
    push.32 dup.1 u32wrapping_sub pow2
    # Stack: [2^(32-m), m, a0, a1, a2, a3, ...]
    # Index:  0         1   2   3   4   5

    # Compute c3 = a3 >> m
    dup.5 dup.2 u32shr          # [c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0    1         2   3   4   5   6

    # Compute c2 = (a2 >> m) | (a3 << (32-m))
    dup.5 dup.3 u32shr          # [a2>>m, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0      1   2         3   4   5   6   7
    dup.7 dup.3 u32widening_mul swap drop  # u32widening_mul: [lo, hi], swap drop keeps lo
    u32or                       # [c2, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0    1   2         3   4   5   6   7

    # Compute c1 = (a1 >> m) | (a2 << (32-m))
    dup.5 dup.4 u32shr          # [a1>>m, c2, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0      1   2   3         4   5   6   7   8
    dup.7 dup.4 u32widening_mul swap drop  # u32widening_mul: [lo, hi], swap drop keeps lo
    u32or                       # [c1, c2, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0    1   2   3         4   5   6   7   8

    # Compute c0 = (a0 >> m) | (a1 << (32-m))
    dup.5 dup.5 u32shr          # [a0>>m, c1, c2, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0      1   2   3   4         5   6   7   8   9
    dup.7 dup.5 u32widening_mul swap drop  # u32widening_mul: [lo, hi], swap drop keeps lo
    u32or                       # [c0, c1, c2, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    #                             0    1   2   3   4         5   6   7   8   9

    # Clean up: move c values to final position, drop temporaries
    # Stack: [c0, c1, c2, c3, 2^(32-m), m, a0, a1, a2, a3, ...]
    movdn.9 movdn.9 movdn.9 movdn.9
    # Stack: [2^(32-m), m, a0, a1, a2, a3, c0, c1, c2, c3, ...]
    drop drop drop drop drop drop
    # Stack: [c0, c1, c2, c3, ...]
end

#! Helper for shr with k=1 (shift by 32+m bits)
#! Input: [m, a0, a1, a2, a3, ...]
#! Output: [c0, c1, c2, c3, ...] where c = a >> (32+m)
#! c0 = (a1 >> m) | (a2 << (32-m))
#! c1 = (a2 >> m) | (a3 << (32-m))
#! c2 = a3 >> m
#! c3 = 0
proc shr_k1
    # Handle m=0 case (shift by exactly 32)
    dup eq.0
    if.true
        drop                    # [a0, a1, a2, a3, ...]
        drop                    # [a1, a2, a3, ...]
        # c0=a1, c1=a2, c2=a3, c3=0 (from padding)
    else
        # Compute 2^(32-m) for left shift operations
        push.32 dup.1 u32wrapping_sub pow2
        # Stack: [2^(32-m), m, a0, a1, a2, a3, ...]

        # c2 = a3 >> m
        dup.5 dup.2 u32shr

        # c1 = (a2 >> m) | (a3 << (32-m))
        dup.5 dup.3 u32shr
        dup.7 dup.3 u32widening_mul swap drop
        u32or

        # c0 = (a1 >> m) | (a2 << (32-m))
        dup.5 dup.4 u32shr
        dup.7 dup.4 u32widening_mul swap drop
        u32or

        # Clean up
        movdn.8 movdn.8 movdn.8
        drop drop drop drop drop drop
        # c0, c1, c2, c3=0 (from padding)
    end
end

#! Helper for shr with k=2 (shift by 64+m bits)
#! Input: [m, a0, a1, a2, a3, ...]
#! Output: [c0, c1, c2, c3, ...] where c = a >> (64+m)
#! c0 = (a2 >> m) | (a3 << (32-m))
#! c1 = a3 >> m
#! c2 = 0, c3 = 0
proc shr_k2
    # Handle m=0 case (shift by exactly 64)
    dup eq.0
    if.true
        drop                    # [a0, a1, a2, a3, ...]
        drop drop               # [a2, a3, ...]
        # c0=a2, c1=a3, c2=c3=0 (from padding)
    else
        # Compute 2^(32-m) for left shift operations
        push.32 dup.1 u32wrapping_sub pow2

        # c1 = a3 >> m
        dup.5 dup.2 u32shr

        # c0 = (a2 >> m) | (a3 << (32-m))
        dup.5 dup.3 u32shr
        dup.7 dup.3 u32widening_mul swap drop
        u32or

        # Clean up
        movdn.7 movdn.7
        drop drop drop drop drop drop
        # c0, c1, c2=c3=0 (from padding)
    end
end

#! Helper for shr with k=3 (shift by 96+m bits)
#! Input: [m, a0, a1, a2, a3, ...]
#! Output: [c0, c1, c2, c3, ...] where c = a >> (96+m)
#! c0 = a3 >> m
#! c1 = 0, c2 = 0, c3 = 0
proc shr_k3
    # c0 = a3 >> m
    movup.4 swap u32shr         # [c0, a0, a1, a2, ...]
    # Clean up: drop a0, a1, a2
    swap drop swap drop swap drop
    # c0, c1=c2=c3=0 (from padding)
end

#! Performs left rotation of one unsigned 128-bit integer.
#! The input value to be rotated is assumed to be represented using 32 bit limbs, but this is not checked.
#! The rotation amount n should be in the range [0, 128), otherwise it will result in an error.
#! Stack transition looks as follows:
#! [n, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = a <<< n (rotate left).
#!
#! rotl(n, a) = (a << n) | (a >> (128-n))
@locals(4)
pub proc rotl(n: u32, a: u128) -> u128
    # Handle n == 0: no rotation needed, value unchanged
    dup eq.0
    if.true
        drop        # [a0, a1, a2, a3, ...]
    else
        # For rotation, we need: (a << n) | (a >> (128-n))
        # Duplicate inputs for both shifts
        dup             # [n, n, a0, a1, a2, a3, ...]
        dup.5 dup.5 dup.5 dup.5     # [a0, a1, a2, a3, n, n, a0, a1, a2, a3, ...]
        movup.5         # [n, a0, a1, a2, a3, n, a0, a1, a2, a3, ...]
        exec.shl        # [shl0, shl1, shl2, shl3, n, a0, a1, a2, a3, ...]

        # Save shl values to local memory so shr can use proper stack padding
        loc_store.0     # store shl0, [shl1, shl2, shl3, n, a0, a1, a2, a3, ...]
        loc_store.1     # store shl1, [shl2, shl3, n, a0, a1, a2, a3, ...]
        loc_store.2     # store shl2, [shl3, n, a0, a1, a2, a3, ...]
        loc_store.3     # store shl3, [n, a0, a1, a2, a3, ...]

        # Now compute a >> (128-n)
        push.128 swap u32wrapping_sub   # [128-n, a0, a1, a2, a3, ...]
        exec.shr        # [shr0, shr1, shr2, shr3, ...]

        # OR with shl values from local memory
        loc_load.0 u32or   # [c0, shr1, shr2, shr3, ...]
        swap
        loc_load.1 u32or   # [c1, c0, shr2, shr3, ...]
        swap               # [c0, c1, shr2, shr3, ...]
        movup.2
        loc_load.2 u32or   # [c2, c0, c1, shr3, ...]
        movdn.2            # [c0, c1, c2, shr3, ...]
        movup.3
        loc_load.3 u32or   # [c3, c0, c1, c2, ...]
        movdn.3            # [c0, c1, c2, c3, ...]
    end
end

#! Performs right rotation of one unsigned 128-bit integer.
#! The input value to be rotated is assumed to be represented using 32 bit limbs, but this is not checked.
#! The rotation amount n should be in the range [0, 128), otherwise it will result in an error.
#! Stack transition looks as follows:
#! [n, a0, a1, a2, a3, ...] -> [c0, c1, c2, c3, ...], where c = a >>> n (rotate right).
#!
#! rotr(n, a) = (a >> n) | (a << (128-n))
@locals(5)
pub proc rotr(n: u32, a: u128) -> u128
    # Handle n == 0: no rotation needed, value unchanged
    dup eq.0
    if.true
        drop        # [a0, a1, a2, a3, ...]
    else
        # For rotation, we need: (a >> n) | (a << (128-n))
        # Save n and a to local memory for later shl computation
        dup loc_store.0      # save n
        dup.1 loc_store.1    # save a0
        dup.2 loc_store.2    # save a1
        dup.3 loc_store.3    # save a2
        dup.4 loc_store.4    # save a3
        # Stack unchanged: [n, a0, a1, a2, a3, ...]

        # Compute shr(n, a) - now stack has proper padding below
        exec.shr        # [shr0, shr1, shr2, shr3, ...]

        # Compute 128-n and prepare for shl
        loc_load.0 push.128 swap u32wrapping_sub  # [128-n, shr0, shr1, shr2, shr3, ...]

        # Load a from memory (in reverse order so they end up in correct position)
        loc_load.4 loc_load.3 loc_load.2 loc_load.1
        # Stack: [a0, a1, a2, a3, 128-n, shr0, shr1, shr2, shr3, ...]
        movup.4         # [128-n, a0, a1, a2, a3, shr0, shr1, shr2, shr3, ...]

        # Compute shl(128-n, a)
        exec.shl        # [shl0, shl1, shl2, shl3, shr0, shr1, shr2, shr3, ...]

        # OR the two results
        movup.4 u32or   # [c0, shl1, shl2, shl3, shr1, shr2, shr3, ...]
        swap movup.4 u32or  # [c1, c0, shl2, shl3, shr2, shr3, ...]
        swap            # [c0, c1, shl2, shl3, shr2, shr3, ...]
        movup.2 movup.4 u32or   # [c2, c0, c1, shl3, shr3, ...]
        movdn.2         # [c0, c1, c2, shl3, shr3, ...]
        movup.3 movup.4 u32or   # [c3, c0, c1, c2, ...]
        movdn.3         # [c0, c1, c2, c3, ...]
    end
end
